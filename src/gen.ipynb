{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "\n",
    "from trl import (\n",
    "    ModelConfig,\n",
    "    RewardConfig,\n",
    "    RewardTrainer,\n",
    "    ScriptArguments,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    "    setup_chat_format,\n",
    ")\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from typing import List, Dict, Any, Union\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_log_softmax(logits, index):\n",
    "    \"\"\"\n",
    "    A memory-efficient implementation of the common `log_softmax -> gather` operation.\n",
    "\n",
    "    This function is equivalent to the following naive implementation:\n",
    "    ```python\n",
    "    logps = torch.gather(logits.log_softmax(-1), dim=-1, index=index.unsqueeze(-1)).squeeze(-1)\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        logits (`torch.Tensor`):\n",
    "            Logits tensor of shape `(..., num_classes)`.\n",
    "        index (`torch.Tensor`):\n",
    "            Index tensor of shape `(...)`, specifying the positions to gather from the log-softmax output.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`:\n",
    "            Gathered log probabilities with the same shape as `index`.\n",
    "    \"\"\"\n",
    "    if logits.dtype in [torch.float32, torch.float64]:\n",
    "        selected_logits = torch.gather(logits, dim=-1, index=index.unsqueeze(-1)).squeeze(-1)\n",
    "        # loop to reduce peak mem consumption\n",
    "        logsumexp_values = torch.stack([torch.logsumexp(lg, dim=-1) for lg in logits])\n",
    "        per_token_logps = selected_logits - logsumexp_values  # log_softmax(x_i) = x_i - logsumexp(x)\n",
    "    else:\n",
    "        # logsumexp approach is unstable with bfloat16, fall back to slightly less efficent approach\n",
    "        per_token_logps = []\n",
    "        for row_logits, row_labels in zip(logits, index):  # loop to reduce peak mem consumption\n",
    "            row_logps = F.log_softmax(row_logits, dim=-1)\n",
    "            row_per_token_logps = row_logps.gather(dim=-1, index=row_labels.unsqueeze(-1)).squeeze(-1)\n",
    "            per_token_logps.append(row_per_token_logps)\n",
    "        per_token_logps = torch.stack(per_token_logps)\n",
    "    return per_token_logps\n",
    "def flush_left(mask: torch.Tensor, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:\n",
    "    \"\"\"\n",
    "    Shift non-zero elements in the mask and corresponding tensors to the left.\n",
    "\n",
    "    This function operates on a binary mask and any number of additional tensors with the same dimensions as the mask.\n",
    "    For each row, non-zero values are shifted to the leftmost positions. Then, columns that contain only zeros across\n",
    "    all rows are truncated from the mask and tensors. Visually, this operation can be represented as follows:\n",
    "\n",
    "    ```\n",
    "    [[0, 0, x, x, x, x],  ->  [[x, x, x, x],\n",
    "     [0, x, x, x, 0, 0]]       [x, x, x, 0]]\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "\n",
    "        mask (`torch.Tensor`):\n",
    "            2D tensor (binary mask) with shape `(N, M)`.\n",
    "        *tensors (`torch.Tensor`)\n",
    "            One or more 2D tensors with the same shape as `mask`. These tensors will be processed alongside `mask`,\n",
    "            with non-zero values shifted and excess zero columns truncated in the same manner.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`:\n",
    "            Updated binary mask with non-zero values flushed to the left and trailing zero columns removed.\n",
    "        `*torch.Tensor`\n",
    "            Updated tensors, processed in the same way as the mask.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    >>> mask = torch.tensor([[0, 0, 1, 1, 1],\n",
    "    ...                      [0, 1, 1, 0, 0]])\n",
    "    >>> tensor = torch.tensor([[9, 9, 2, 3, 4],\n",
    "    ...                        [9, 5, 6, 9, 9]])\n",
    "    >>> new_mask, new_tensor = flush_left(mask, tensor)\n",
    "    >>> print(new_mask)\n",
    "    tensor([[1, 1, 1],\n",
    "            [1, 1, 0]])\n",
    "    >>> print(new_tensor)\n",
    "    tensor([[2, 3, 4],\n",
    "            [5, 6, 0]])\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Create copy of mask and tensors\n",
    "    mask = mask.clone()\n",
    "    tensors = [t.clone() for t in tensors]\n",
    "\n",
    "    # Shift non-zero values to the left\n",
    "    for i in range(mask.size(0)):\n",
    "        first_one_idx = torch.nonzero(mask[i])[0].item()\n",
    "        mask[i] = torch.roll(mask[i], shifts=-first_one_idx)\n",
    "        for tensor in tensors:\n",
    "            tensor[i] = torch.roll(tensor[i], shifts=-first_one_idx)\n",
    "\n",
    "    # Get the first column idx that is all zeros and remove every column after that\n",
    "    empty_cols = torch.sum(mask, dim=0) == 0\n",
    "    first_empty_col = torch.nonzero(empty_cols)[0].item() if empty_cols.any() else mask.size(1)\n",
    "    mask = mask[:, :first_empty_col]\n",
    "    for i, tensor in enumerate(tensors):\n",
    "        tensors[i] = tensor[:, :first_empty_col]\n",
    "\n",
    "    if not tensors:\n",
    "        return mask\n",
    "    else:\n",
    "        return mask, *tensors\n",
    "\n",
    "\n",
    "def pad(tensors: list[torch.Tensor], padding_value: int = 0, padding_side: str = \"right\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pads a list of tensors to the same shape along the first dimension.\n",
    "\n",
    "    Args:\n",
    "        tensors (`list[torch.Tensor]`):\n",
    "            List of input tensors to pad.\n",
    "        padding_value (`int`):\n",
    "            Value to use for padding. Default is 0.\n",
    "        padding_side (`str`):\n",
    "            Side on which to add padding. Must be 'left' or 'right'. Default is 'right'.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`:\n",
    "            A single tensor containing the padded tensors.\n",
    "\n",
    "    Examples:\n",
    "        >>> import torch\n",
    "        >>> pad([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n",
    "        tensor([[1, 2, 3],\n",
    "                [4, 5, 0]])\n",
    "        >>> pad([torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6]])])\n",
    "        tensor([[[1, 2],\n",
    "                [3, 4]],\n",
    "\n",
    "                [[5, 6],\n",
    "                [0, 0]]])\n",
    "    \"\"\"\n",
    "    # Determine the maximum shape for each dimension\n",
    "    output_shape = np.max([t.shape for t in tensors], 0).tolist()\n",
    "\n",
    "    # Create an output tensor filled with the padding value\n",
    "    output = torch.full((len(tensors), *output_shape), padding_value, dtype=tensors[0].dtype, device=tensors[0].device)\n",
    "\n",
    "    for i, t in enumerate(tensors):\n",
    "        # Determine the slice for the sequence dimension\n",
    "        if padding_side == \"left\":\n",
    "            seq_slice = slice(output_shape[0] - t.shape[0], output_shape[0])\n",
    "        elif padding_side == \"right\":\n",
    "            seq_slice = slice(0, t.shape[0])\n",
    "        else:\n",
    "            raise ValueError(\"padding_side must be 'left' or 'right'\")\n",
    "\n",
    "        slices = (seq_slice,) + tuple(slice(0, s) for s in t.shape[1:])\n",
    "        output[i][slices] = t\n",
    "\n",
    "    return output\n",
    "def torch_call(examples: list[Union[list[int], Any, dict[str, Any]]], pad_token_id) -> dict[str, Any]:\n",
    "        # Convert to tensor\n",
    "        prompt_input_ids = [torch.tensor(example[\"prompt_input_ids\"]) for example in examples]\n",
    "        prompt_attention_mask = [torch.ones_like(input_ids) for input_ids in prompt_input_ids]\n",
    "        chosen_input_ids = [torch.tensor(example[\"chosen_input_ids\"]) for example in examples]\n",
    "        chosen_attention_mask = [torch.ones_like(input_ids) for input_ids in chosen_input_ids]\n",
    "        rejected_input_ids = [torch.tensor(example[\"rejected_input_ids\"]) for example in examples]\n",
    "        rejected_attention_mask = [torch.ones_like(input_ids) for input_ids in rejected_input_ids]\n",
    "        if \"pixel_values\" in examples[0]:\n",
    "            pixel_values = [torch.tensor(example[\"pixel_values\"]) for example in examples]\n",
    "        if \"pixel_attention_mask\" in examples[0]:\n",
    "            pixel_attention_mask = [torch.tensor(example[\"pixel_attention_mask\"]) for example in examples]\n",
    "        if \"ref_chosen_logps\" in examples[0] and \"ref_rejected_logps\" in examples[0]:\n",
    "            ref_chosen_logps = torch.tensor([example[\"ref_chosen_logps\"] for example in examples])\n",
    "            ref_rejected_logps = torch.tensor([example[\"ref_rejected_logps\"] for example in examples])\n",
    "\n",
    "        # Pad\n",
    "        output = {}\n",
    "        output[\"prompt_input_ids\"] = pad(prompt_input_ids, padding_value=pad_token_id, padding_side=\"left\")\n",
    "        output[\"prompt_attention_mask\"] = pad(prompt_attention_mask, padding_value=0, padding_side=\"left\")\n",
    "        output[\"chosen_input_ids\"] = pad(chosen_input_ids, padding_value=pad_token_id)\n",
    "        output[\"chosen_attention_mask\"] = pad(chosen_attention_mask, padding_value=0)\n",
    "        output[\"rejected_input_ids\"] = pad(rejected_input_ids, padding_value=pad_token_id)\n",
    "        output[\"rejected_attention_mask\"] = pad(rejected_attention_mask, padding_value=0)\n",
    "        if \"pixel_values\" in examples[0]:\n",
    "            output[\"pixel_values\"] = pad(pixel_values, padding_value=0.0)\n",
    "        if \"pixel_attention_mask\" in examples[0]:\n",
    "            output[\"pixel_attention_mask\"] = pad(pixel_attention_mask, padding_value=0)\n",
    "        if \"image_sizes\" in examples[0]:\n",
    "            output[\"image_sizes\"] = torch.tensor([example[\"image_sizes\"] for example in examples])\n",
    "        if \"ref_chosen_logps\" in examples[0] and \"ref_rejected_logps\" in examples[0]:\n",
    "            output[\"ref_chosen_logps\"] = ref_chosen_logps\n",
    "            output[\"ref_rejected_logps\"] = ref_rejected_logps\n",
    "\n",
    "        return output\n",
    "def pad_to_length(tensor: torch.Tensor, length: int, pad_value: Union[int, float], dim: int = -1) -> torch.Tensor:\n",
    "    if tensor.size(dim) >= length:\n",
    "        return tensor\n",
    "    else:\n",
    "        pad_size = list(tensor.shape)\n",
    "        pad_size[dim] = length - tensor.size(dim)\n",
    "        return torch.cat(\n",
    "            [\n",
    "                tensor,\n",
    "                pad_value * torch.ones(*pad_size, dtype=tensor.dtype, device=tensor.device),\n",
    "            ],\n",
    "            dim=dim,\n",
    "        )\n",
    "def concatenated_inputs(\n",
    "    batch: dict[str, Union[list, torch.LongTensor]], padding_value: int\n",
    ") -> dict[str, torch.LongTensor]:\n",
    "    \"\"\"\n",
    "    Concatenate the `chosen` and `rejected` inputs from the batch into a single tensor for both the prompt\n",
    "    and completion sequences.\n",
    "\n",
    "    Args:\n",
    "        batch (`dict[str, Union[list, torch.LongTensor]]`):\n",
    "            A batch of input data. The batch must contain the following keys:\n",
    "\n",
    "            - `\"prompt_input_ids\"`: Tensor of shape `(batch_size, prompt_length)` representing the prompt input IDs.\n",
    "            - `\"chosen_input_ids\"`: Tensor of shape `(batch_size, chosen_length)` representing the chosen completion input IDs.\n",
    "            - `\"rejected_input_ids\"`: Tensor of shape `(batch_size, rejected_length)` representing the rejected completion input IDs.\n",
    "            - `\"prompt_pixel_values\"` (optional): Tensor for pixel values, if available.\n",
    "            - `\"prompt_pixel_attention_mask\"` (optional): Tensor for pixel attention masks, if available.\n",
    "\n",
    "        padding_value (`int`):\n",
    "            The padding value to use for the concatenated completion sequences (`chosen_input_ids` and\n",
    "            `rejected_input_ids`).\n",
    "\n",
    "    Returns:\n",
    "        `dict[str, torch.LongTensor]`: A dictionary containing:\n",
    "\n",
    "            - `\"prompt_input_ids\"`: Concatenated prompt input IDs of shape `(2 * batch_size, prompt_length)`.\n",
    "            - `\"completion_input_ids\"`: Concatenated chosen and rejected completion input IDs of shape `(2 * batch_size, max_completion_length)`.\n",
    "            - `\"prompt_attention_mask\"`: Concatenated prompt attention masks of shape `(2 * batch_size, prompt_length)`.\n",
    "            - `\"completion_attention_mask\"`: Concatenated chosen and rejected attention masks of shape `(2 * batch_size, max_completion_length)`.\n",
    "            - `\"pixel_values\"` (optional): Concatenated pixel values if `\"prompt_pixel_values\"` are present.\n",
    "            - `\"pixel_attention_mask\"` (optional): Concatenated pixel attention masks if `\"prompt_pixel_attention_mask\"` are present.\n",
    "\n",
    "    Notes:\n",
    "        The completion input IDs and attention masks are padded to the maximum completion length of the chosen\n",
    "        or rejected sequences.\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "\n",
    "    # For the prompt, the input_ids are the same for both the chosen and rejected responses\n",
    "    output[\"prompt_input_ids\"] = torch.cat([batch[\"prompt_input_ids\"], batch[\"prompt_input_ids\"]], dim=0)\n",
    "    output[\"prompt_attention_mask\"] = torch.cat(\n",
    "        [batch[\"prompt_attention_mask\"], batch[\"prompt_attention_mask\"]], dim=0\n",
    "    )\n",
    "    if \"pixel_values\" in batch:\n",
    "        output[\"pixel_values\"] = torch.cat([batch[\"pixel_values\"], batch[\"pixel_values\"]], dim=0)\n",
    "\n",
    "    if \"pixel_attention_mask\" in batch:\n",
    "        output[\"pixel_attention_mask\"] = torch.cat(\n",
    "            [batch[\"pixel_attention_mask\"], batch[\"pixel_attention_mask\"]], dim=0\n",
    "        )\n",
    "    if \"image_sizes\" in batch:\n",
    "        output[\"image_sizes\"] = torch.cat([batch[\"image_sizes\"], batch[\"image_sizes\"]], dim=0)\n",
    "\n",
    "    # Concatenate the chosen and rejected completions\n",
    "    max_completion_length = max(batch[\"chosen_input_ids\"].shape[1], batch[\"rejected_input_ids\"].shape[1])\n",
    "    output[\"completion_input_ids\"] = torch.cat(\n",
    "        (\n",
    "            pad_to_length(batch[\"chosen_input_ids\"], max_completion_length, pad_value=padding_value),\n",
    "            pad_to_length(batch[\"rejected_input_ids\"], max_completion_length, pad_value=padding_value),\n",
    "        ),\n",
    "    )\n",
    "    output[\"completion_attention_mask\"] = torch.cat(\n",
    "        (\n",
    "            pad_to_length(batch[\"chosen_attention_mask\"], max_completion_length, pad_value=0),\n",
    "            pad_to_length(batch[\"rejected_attention_mask\"], max_completion_length, pad_value=0),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"Hankbeasley/polycoder\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    parts = examples['accept'].split('<think>')\n",
    "    # if (len(parts) < 2 or len(parts) > 2):\n",
    "    #     return None\n",
    "    assistantparts = examples['accept'].split(\"<｜Assistant｜>\")\n",
    "    # if (len(assistantparts) > 2):\n",
    "    #     return None\n",
    "    examples['prompt'] = parts[0]\n",
    "    examples['chosen'] = parts[1]\n",
    "    partsrejected = examples['reject'].split('<think>')\n",
    "    examples['rejected'] = \"<think>\" + partsrejected[1]\n",
    "    return examples\n",
    "ds = ds.map(preprocess_function, num_proc=12)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    accepts = tokenizer(examples[\"chosen\"], truncation=True, padding=True)\n",
    "    rejects = tokenizer(examples[\"rejected\"], truncation=True, padding=True)\n",
    "    prompts = tokenizer(examples[\"prompt\"], truncation=True, padding=True)\n",
    "    \n",
    "    return {\n",
    "        \"chosen_input_ids\": accepts[\"input_ids\"],\n",
    "        \"chosen_attention_mask\": accepts[\"attention_mask\"],\n",
    "        \"rejected_input_ids\": rejects[\"input_ids\"],\n",
    "        \"rejected_attention_mask\": rejects[\"attention_mask\"],\n",
    "        \"prompt_input_ids\": prompts[\"input_ids\"],\n",
    "        \"prompt_attention_mask\": prompts[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "tokenized_ds = ds['train'].map(tokenize_function, batched=False, batch_size=100, num_proc=12)\n",
    "tokenized_ds=tokenized_ds.remove_columns(['accept', 'reject', 'prompt', 'chosen', 'rejected','testname'])\n",
    "tokenized_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = tokenized_ds.select(range(1))\n",
    "train\n",
    "t = torch_call(train, 0)\n",
    "#train = tokenized_ds[:10]\n",
    "#train\n",
    "#train[\"prompt_input_ids\"]\n",
    "#len(train[\"prompt_input_ids\"])\n",
    "#t = torch_call(train, 0)\n",
    "#t\n",
    "concatenated_batch = concatenated_inputs(t, 0)\n",
    "concatenated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input_ids = concatenated_batch[\"prompt_input_ids\"]\n",
    "prompt_attention_mask = concatenated_batch[\"prompt_attention_mask\"]\n",
    "completion_input_ids = concatenated_batch[\"completion_input_ids\"]\n",
    "completion_attention_mask = concatenated_batch[\"completion_attention_mask\"]\n",
    "\n",
    "input_ids = torch.cat((prompt_input_ids, completion_input_ids), dim=1)\n",
    "attention_mask = torch.cat((prompt_attention_mask, completion_attention_mask), dim=1)\n",
    "loss_mask = torch.cat(\n",
    "                (torch.zeros_like(prompt_attention_mask), completion_attention_mask),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "# Flush left to reduce the memory usage\n",
    "# [[0, 0, x, x, x, x],  ->  [[x, x, x, x],\n",
    "#  [0, x, x, x, 0, 0]]       [x, x, x, 0]]\n",
    "attention_mask, input_ids, loss_mask = flush_left(attention_mask, input_ids, loss_mask)\n",
    "# attention_mask.to(\"cuda\")\n",
    "# input_ids.to(\"cuda\")\n",
    "# loss_mask.to(\"cuda\")\n",
    "len(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_kwargs = {}\n",
    "model_kwargs[\"attention_mask\"] = attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision\n",
    "#modelid = \"Hankbeasley/PolycrestSFT-Qwen-7B\"\n",
    "\n",
    "\n",
    "\n",
    "modelid = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(modelid, \n",
    "                                             device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    torch_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "print(device)\n",
    "inputs = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "outputs = model(inputs, **model_kwargs)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = torch.roll(input_ids, shifts=-1, dims=1).to(device)\n",
    "loss_mask = torch.roll(loss_mask, shifts=-1, dims=1).bool().to(device)\n",
    "per_token_logps = selective_log_softmax(logits, labels)\n",
    "print(per_token_logps)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    parts = examples['accept'].split('<think>')\n",
    "    if (len(parts) < 2 or len(parts) > 2):\n",
    "        return None\n",
    "    assistantparts = examples['accept'].split(\"<｜Assistant｜>\")\n",
    "    if (len(assistantparts) > 2):\n",
    "        return None\n",
    "    examples['prompt'] = parts[0]\n",
    "    examples['chosen'] = parts[1]\n",
    "    partsrejected = examples['reject'].split('<think>')\n",
    "    examples['rejected'] = \"<think>\" + partsrejected[1]\n",
    "    return examples\n",
    "\n",
    "ds = ds['train'].map(preprocess_function)\n",
    "#print(ds)\n",
    "formated = ds.remove_columns(['accept', 'reject', 'testname'])\n",
    "print(formated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(formated[0]['prompt'])\n",
    "#print(formated)\n",
    "parts = ds[0]['accept'].split('<think>')\n",
    "prompt = ds[0]['accept'].split('<think>')[0]\n",
    "#print(parts[1])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "tokenized = tokenizer(formated[0]['rejected'], add_special_tokens=True)\n",
    "print(tokenized[\"input_ids\"])\n",
    "#print(tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"]))\n",
    "#print(tokenizer.eos_token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
