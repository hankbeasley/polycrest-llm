{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Union\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "def selective_log_softmax(logits, index):\n",
    "    \"\"\"\n",
    "    A memory-efficient implementation of the common `log_softmax -> gather` operation.\n",
    "\n",
    "    This function is equivalent to the following naive implementation:\n",
    "    ```python\n",
    "    logps = torch.gather(logits.log_softmax(-1), dim=-1, index=index.unsqueeze(-1)).squeeze(-1)\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        logits (`torch.Tensor`):\n",
    "            Logits tensor of shape `(..., num_classes)`.\n",
    "        index (`torch.Tensor`):\n",
    "            Index tensor of shape `(...)`, specifying the positions to gather from the log-softmax output.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`:\n",
    "            Gathered log probabilities with the same shape as `index`.\n",
    "    \"\"\"\n",
    "    if logits.dtype in [torch.float32, torch.float64]:\n",
    "        selected_logits = torch.gather(logits, dim=-1, index=index.unsqueeze(-1)).squeeze(-1)\n",
    "        # loop to reduce peak mem consumption\n",
    "        logsumexp_values = torch.stack([torch.logsumexp(lg, dim=-1) for lg in logits])\n",
    "        per_token_logps = selected_logits - logsumexp_values  # log_softmax(x_i) = x_i - logsumexp(x)\n",
    "    else:\n",
    "        # logsumexp approach is unstable with bfloat16, fall back to slightly less efficent approach\n",
    "        per_token_logps = []\n",
    "        for row_logits, row_labels in zip(logits, index):  # loop to reduce peak mem consumption\n",
    "            row_logps = F.log_softmax(row_logits, dim=-1)\n",
    "            row_per_token_logps = row_logps.gather(dim=-1, index=row_labels.unsqueeze(-1)).squeeze(-1)\n",
    "            per_token_logps.append(row_per_token_logps)\n",
    "        per_token_logps = torch.stack(per_token_logps)\n",
    "    return per_token_logps\n",
    "def flush_left(mask: torch.Tensor, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:\n",
    "    \"\"\"\n",
    "    Shift non-zero elements in the mask and corresponding tensors to the left.\n",
    "\n",
    "    This function operates on a binary mask and any number of additional tensors with the same dimensions as the mask.\n",
    "    For each row, non-zero values are shifted to the leftmost positions. Then, columns that contain only zeros across\n",
    "    all rows are truncated from the mask and tensors. Visually, this operation can be represented as follows:\n",
    "\n",
    "    ```\n",
    "    [[0, 0, x, x, x, x],  ->  [[x, x, x, x],\n",
    "     [0, x, x, x, 0, 0]]       [x, x, x, 0]]\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "\n",
    "        mask (`torch.Tensor`):\n",
    "            2D tensor (binary mask) with shape `(N, M)`.\n",
    "        *tensors (`torch.Tensor`)\n",
    "            One or more 2D tensors with the same shape as `mask`. These tensors will be processed alongside `mask`,\n",
    "            with non-zero values shifted and excess zero columns truncated in the same manner.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`:\n",
    "            Updated binary mask with non-zero values flushed to the left and trailing zero columns removed.\n",
    "        `*torch.Tensor`\n",
    "            Updated tensors, processed in the same way as the mask.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    >>> mask = torch.tensor([[0, 0, 1, 1, 1],\n",
    "    ...                      [0, 1, 1, 0, 0]])\n",
    "    >>> tensor = torch.tensor([[9, 9, 2, 3, 4],\n",
    "    ...                        [9, 5, 6, 9, 9]])\n",
    "    >>> new_mask, new_tensor = flush_left(mask, tensor)\n",
    "    >>> print(new_mask)\n",
    "    tensor([[1, 1, 1],\n",
    "            [1, 1, 0]])\n",
    "    >>> print(new_tensor)\n",
    "    tensor([[2, 3, 4],\n",
    "            [5, 6, 0]])\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Create copy of mask and tensors\n",
    "    mask = mask.clone()\n",
    "    tensors = [t.clone() for t in tensors]\n",
    "\n",
    "    # Shift non-zero values to the left\n",
    "    for i in range(mask.size(0)):\n",
    "        first_one_idx = torch.nonzero(mask[i])[0].item()\n",
    "        mask[i] = torch.roll(mask[i], shifts=-first_one_idx)\n",
    "        for tensor in tensors:\n",
    "            tensor[i] = torch.roll(tensor[i], shifts=-first_one_idx)\n",
    "\n",
    "    # Get the first column idx that is all zeros and remove every column after that\n",
    "    empty_cols = torch.sum(mask, dim=0) == 0\n",
    "    first_empty_col = torch.nonzero(empty_cols)[0].item() if empty_cols.any() else mask.size(1)\n",
    "    mask = mask[:, :first_empty_col]\n",
    "    for i, tensor in enumerate(tensors):\n",
    "        tensors[i] = tensor[:, :first_empty_col]\n",
    "\n",
    "    if not tensors:\n",
    "        return mask\n",
    "    else:\n",
    "        return mask, *tensors\n",
    "\n",
    "\n",
    "def pad(tensors: list[torch.Tensor], padding_value: int = 0, padding_side: str = \"right\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pads a list of tensors to the same shape along the first dimension.\n",
    "\n",
    "    Args:\n",
    "        tensors (`list[torch.Tensor]`):\n",
    "            List of input tensors to pad.\n",
    "        padding_value (`int`):\n",
    "            Value to use for padding. Default is 0.\n",
    "        padding_side (`str`):\n",
    "            Side on which to add padding. Must be 'left' or 'right'. Default is 'right'.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`:\n",
    "            A single tensor containing the padded tensors.\n",
    "\n",
    "    Examples:\n",
    "        >>> import torch\n",
    "        >>> pad([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n",
    "        tensor([[1, 2, 3],\n",
    "                [4, 5, 0]])\n",
    "        >>> pad([torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6]])])\n",
    "        tensor([[[1, 2],\n",
    "                [3, 4]],\n",
    "\n",
    "                [[5, 6],\n",
    "                [0, 0]]])\n",
    "    \"\"\"\n",
    "    # Determine the maximum shape for each dimension\n",
    "    output_shape = np.max([t.shape for t in tensors], 0).tolist()\n",
    "\n",
    "    # Create an output tensor filled with the padding value\n",
    "    output = torch.full((len(tensors), *output_shape), padding_value, dtype=tensors[0].dtype, device=tensors[0].device)\n",
    "\n",
    "    for i, t in enumerate(tensors):\n",
    "        # Determine the slice for the sequence dimension\n",
    "        if padding_side == \"left\":\n",
    "            seq_slice = slice(output_shape[0] - t.shape[0], output_shape[0])\n",
    "        elif padding_side == \"right\":\n",
    "            seq_slice = slice(0, t.shape[0])\n",
    "        else:\n",
    "            raise ValueError(\"padding_side must be 'left' or 'right'\")\n",
    "\n",
    "        slices = (seq_slice,) + tuple(slice(0, s) for s in t.shape[1:])\n",
    "        output[i][slices] = t\n",
    "\n",
    "    return output\n",
    "def torch_call(examples: list[Union[list[int], Any, dict[str, Any]]], pad_token_id) -> dict[str, Any]:\n",
    "        # Convert to tensor\n",
    "        prompt_input_ids = [torch.tensor(example[\"prompt_input_ids\"]) for example in examples]\n",
    "        prompt_attention_mask = [torch.ones_like(input_ids) for input_ids in prompt_input_ids]\n",
    "        chosen_input_ids = [torch.tensor(example[\"chosen_input_ids\"]) for example in examples]\n",
    "        chosen_attention_mask = [torch.ones_like(input_ids) for input_ids in chosen_input_ids]\n",
    "        rejected_input_ids = [torch.tensor(example[\"rejected_input_ids\"]) for example in examples]\n",
    "        rejected_attention_mask = [torch.ones_like(input_ids) for input_ids in rejected_input_ids]\n",
    "        if \"pixel_values\" in examples[0]:\n",
    "            pixel_values = [torch.tensor(example[\"pixel_values\"]) for example in examples]\n",
    "        if \"pixel_attention_mask\" in examples[0]:\n",
    "            pixel_attention_mask = [torch.tensor(example[\"pixel_attention_mask\"]) for example in examples]\n",
    "        if \"ref_chosen_logps\" in examples[0] and \"ref_rejected_logps\" in examples[0]:\n",
    "            ref_chosen_logps = torch.tensor([example[\"ref_chosen_logps\"] for example in examples])\n",
    "            ref_rejected_logps = torch.tensor([example[\"ref_rejected_logps\"] for example in examples])\n",
    "\n",
    "        # Pad\n",
    "        output = {}\n",
    "        output[\"prompt_input_ids\"] = pad(prompt_input_ids, padding_value=pad_token_id, padding_side=\"left\")\n",
    "        output[\"prompt_attention_mask\"] = pad(prompt_attention_mask, padding_value=0, padding_side=\"left\")\n",
    "        output[\"chosen_input_ids\"] = pad(chosen_input_ids, padding_value=pad_token_id)\n",
    "        output[\"chosen_attention_mask\"] = pad(chosen_attention_mask, padding_value=0)\n",
    "        output[\"rejected_input_ids\"] = pad(rejected_input_ids, padding_value=pad_token_id)\n",
    "        output[\"rejected_attention_mask\"] = pad(rejected_attention_mask, padding_value=0)\n",
    "        if \"pixel_values\" in examples[0]:\n",
    "            output[\"pixel_values\"] = pad(pixel_values, padding_value=0.0)\n",
    "        if \"pixel_attention_mask\" in examples[0]:\n",
    "            output[\"pixel_attention_mask\"] = pad(pixel_attention_mask, padding_value=0)\n",
    "        if \"image_sizes\" in examples[0]:\n",
    "            output[\"image_sizes\"] = torch.tensor([example[\"image_sizes\"] for example in examples])\n",
    "        if \"ref_chosen_logps\" in examples[0] and \"ref_rejected_logps\" in examples[0]:\n",
    "            output[\"ref_chosen_logps\"] = ref_chosen_logps\n",
    "            output[\"ref_rejected_logps\"] = ref_rejected_logps\n",
    "\n",
    "        return output\n",
    "def pad_to_length(tensor: torch.Tensor, length: int, pad_value: Union[int, float], dim: int = -1) -> torch.Tensor:\n",
    "    if tensor.size(dim) >= length:\n",
    "        return tensor\n",
    "    else:\n",
    "        pad_size = list(tensor.shape)\n",
    "        pad_size[dim] = length - tensor.size(dim)\n",
    "        return torch.cat(\n",
    "            [\n",
    "                tensor,\n",
    "                pad_value * torch.ones(*pad_size, dtype=tensor.dtype, device=tensor.device),\n",
    "            ],\n",
    "            dim=dim,\n",
    "        )\n",
    "def concatenated_inputs(\n",
    "    batch: dict[str, Union[list, torch.LongTensor]], padding_value: int\n",
    ") -> dict[str, torch.LongTensor]:\n",
    "    \"\"\"\n",
    "    Concatenate the `chosen` and `rejected` inputs from the batch into a single tensor for both the prompt\n",
    "    and completion sequences.\n",
    "\n",
    "    Args:\n",
    "        batch (`dict[str, Union[list, torch.LongTensor]]`):\n",
    "            A batch of input data. The batch must contain the following keys:\n",
    "\n",
    "            - `\"prompt_input_ids\"`: Tensor of shape `(batch_size, prompt_length)` representing the prompt input IDs.\n",
    "            - `\"chosen_input_ids\"`: Tensor of shape `(batch_size, chosen_length)` representing the chosen completion input IDs.\n",
    "            - `\"rejected_input_ids\"`: Tensor of shape `(batch_size, rejected_length)` representing the rejected completion input IDs.\n",
    "            - `\"prompt_pixel_values\"` (optional): Tensor for pixel values, if available.\n",
    "            - `\"prompt_pixel_attention_mask\"` (optional): Tensor for pixel attention masks, if available.\n",
    "\n",
    "        padding_value (`int`):\n",
    "            The padding value to use for the concatenated completion sequences (`chosen_input_ids` and\n",
    "            `rejected_input_ids`).\n",
    "\n",
    "    Returns:\n",
    "        `dict[str, torch.LongTensor]`: A dictionary containing:\n",
    "\n",
    "            - `\"prompt_input_ids\"`: Concatenated prompt input IDs of shape `(2 * batch_size, prompt_length)`.\n",
    "            - `\"completion_input_ids\"`: Concatenated chosen and rejected completion input IDs of shape `(2 * batch_size, max_completion_length)`.\n",
    "            - `\"prompt_attention_mask\"`: Concatenated prompt attention masks of shape `(2 * batch_size, prompt_length)`.\n",
    "            - `\"completion_attention_mask\"`: Concatenated chosen and rejected attention masks of shape `(2 * batch_size, max_completion_length)`.\n",
    "            - `\"pixel_values\"` (optional): Concatenated pixel values if `\"prompt_pixel_values\"` are present.\n",
    "            - `\"pixel_attention_mask\"` (optional): Concatenated pixel attention masks if `\"prompt_pixel_attention_mask\"` are present.\n",
    "\n",
    "    Notes:\n",
    "        The completion input IDs and attention masks are padded to the maximum completion length of the chosen\n",
    "        or rejected sequences.\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "\n",
    "    # For the prompt, the input_ids are the same for both the chosen and rejected responses\n",
    "    output[\"prompt_input_ids\"] = torch.cat([batch[\"prompt_input_ids\"], batch[\"prompt_input_ids\"]], dim=0)\n",
    "    output[\"prompt_attention_mask\"] = torch.cat(\n",
    "        [batch[\"prompt_attention_mask\"], batch[\"prompt_attention_mask\"]], dim=0\n",
    "    )\n",
    "    if \"pixel_values\" in batch:\n",
    "        output[\"pixel_values\"] = torch.cat([batch[\"pixel_values\"], batch[\"pixel_values\"]], dim=0)\n",
    "\n",
    "    if \"pixel_attention_mask\" in batch:\n",
    "        output[\"pixel_attention_mask\"] = torch.cat(\n",
    "            [batch[\"pixel_attention_mask\"], batch[\"pixel_attention_mask\"]], dim=0\n",
    "        )\n",
    "    if \"image_sizes\" in batch:\n",
    "        output[\"image_sizes\"] = torch.cat([batch[\"image_sizes\"], batch[\"image_sizes\"]], dim=0)\n",
    "\n",
    "    # Concatenate the chosen and rejected completions\n",
    "    max_completion_length = max(batch[\"chosen_input_ids\"].shape[1], batch[\"rejected_input_ids\"].shape[1])\n",
    "    output[\"completion_input_ids\"] = torch.cat(\n",
    "        (\n",
    "            pad_to_length(batch[\"chosen_input_ids\"], max_completion_length, pad_value=padding_value),\n",
    "            pad_to_length(batch[\"rejected_input_ids\"], max_completion_length, pad_value=padding_value),\n",
    "        ),\n",
    "    )\n",
    "    output[\"completion_attention_mask\"] = torch.cat(\n",
    "        (\n",
    "            pad_to_length(batch[\"chosen_attention_mask\"], max_completion_length, pad_value=0),\n",
    "            pad_to_length(batch[\"rejected_attention_mask\"], max_completion_length, pad_value=0),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function2(examples):\n",
    "    parts = examples['accept'].split('<think>')\n",
    "    partsrejected = examples['reject'].split('<think>')\n",
    "    examples['prompt'] = parts[0]\n",
    "    examples['chosen'] = \"<think>\" + parts[1]\n",
    "    examples['rejected'] = \"<think>\\n\" + partsrejected[1]\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 310 unique prompts\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"Hankbeasley/testds\")\n",
    "train = ds['train']\n",
    "\n",
    "current = load_dataset(\"Hankbeasley/polycoder\")['train']\n",
    "current = current.filter(lambda x: len(x['accept'].split(\"<｜Assistant｜>\"))==2)\n",
    "dsinput = current.map(preprocess_function2)\n",
    "\n",
    "# Create a set of unique prompts and keep track of their indices\n",
    "seen_prompts = set()\n",
    "unique_indices = []\n",
    "\n",
    "for i, example in enumerate(dsinput):\n",
    "    prompt = tuple(example['prompt'])  # Convert to tuple since lists aren't hashable\n",
    "    if prompt not in seen_prompts:\n",
    "        seen_prompts.add(prompt)\n",
    "        unique_indices.append(i)\n",
    "\n",
    "# Get the unique rows with all columns\n",
    "unique_dataset = dsinput.select(unique_indices)\n",
    "print(f\"Found {len(unique_indices)} unique prompts\")\n",
    "#unique_dataset.push_to_hub(\"Hankbeasley/polycodertext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "def tokenize_sample(x):\n",
    "    prompt_tokens = tokenizer(x['prompt'], return_tensors=None, add_special_tokens=False)\n",
    "    chosen_tokens = tokenizer(x['chosen'], return_tensors=None, add_special_tokens=False)\n",
    "    rejected_tokens = tokenizer(x['rejected'], return_tensors=None, add_special_tokens=False)\n",
    "    \n",
    "    # Assumes that token dicts have the same keys (e.g., 'input_ids', 'attention_mask')\n",
    "    combined_tokens = {\n",
    "        key: prompt_tokens[key] + chosen_tokens[key]\n",
    "        for key in prompt_tokens.keys()\n",
    "    }\n",
    "    combined_rejected_tokens = {\n",
    "        key: prompt_tokens[key] + rejected_tokens[key]\n",
    "        for key in prompt_tokens.keys()\n",
    "    }\n",
    "    return {\n",
    "        'prompt_tokens': prompt_tokens,\n",
    "        'chosen_tokens': chosen_tokens,\n",
    "        'rejected_tokens': rejected_tokens,\n",
    "        'combined_tokens': combined_tokens,\n",
    "        'combined_rejected_tokens': combined_rejected_tokens\n",
    "    }\n",
    "\n",
    "\n",
    "cleanset = load_dataset(\"Hankbeasley/polycodertext\")['train']\n",
    "incodedClean = cleanset.map(tokenize_sample)\n",
    "incodedClean = incodedClean.filter(\n",
    "    lambda x: len(x['combined_tokens']['input_ids']) < 13000 and len(x['combined_rejected_tokens']['input_ids']) < 13000\n",
    ")\n",
    "ptokens = incodedClean['prompt_tokens']\n",
    "decoded_again = [\n",
    "    tokenizer.decode(tokens['input_ids'], skip_special_tokens=True)\n",
    "    for tokens in incodedClean['combined_tokens']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generates(model, input_ids, attention_mask):\n",
    "    with torch.inference_mode():\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(model.device)\n",
    "        attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(model.device)\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        del input_ids\n",
    "        del attention_mask\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getloss(output, input_ids, prompt_attention_mask, completion_attention_mask, device):\n",
    "\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "    prompt_attention_mask = torch.tensor(prompt_attention_mask).unsqueeze(0)\n",
    "    completion_attention_mask =torch.tensor(completion_attention_mask).unsqueeze(0)\n",
    "\n",
    "\n",
    "    logits = output.logits\n",
    "    loss_mask = torch.cat(\n",
    "                (torch.zeros_like(prompt_attention_mask), completion_attention_mask),\n",
    "                dim=1,\n",
    "            )\n",
    "    labels = torch.roll(input_ids, shifts=-1, dims=1).to(device)\n",
    "    loss_mask = torch.roll(loss_mask, shifts=-1, dims=1).bool().to(device)\n",
    "    per_token_logps = selective_log_softmax(logits, labels)\n",
    "    loss_mask = loss_mask.to(logits.device)  # Ensure same device\n",
    "    masked_logps = per_token_logps * loss_mask  \n",
    "    total_loss = masked_logps.sum() / loss_mask.sum()  # Average over non-padding tokens\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    per_token_logps[~loss_mask] = 0\n",
    "    per_token_logps = torch.roll(per_token_logps, shifts=1, dims=1)\n",
    "    all_logps = per_token_logps.sum(-1)\n",
    "    num_examples = 1 \n",
    "    output = {}\n",
    "    output[\"chosen_logps\"] = all_logps[:num_examples]\n",
    "    output[\"mean_chosen_logits\"] = logits[:num_examples][loss_mask[:num_examples]].mean()\n",
    "    del input_ids\n",
    "    del prompt_attention_mask\n",
    "    del completion_attention_mask\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    # output[\"chosen_logps\"] = all_logps[:num_examples]\n",
    "    # output[\"rejected_logps\"] = all_logps[num_examples:]\n",
    "    # output[\"mean_chosen_logits\"] = logits[:num_examples][loss_mask[:num_examples]].mean()\n",
    "    # output[\"mean_rejected_logits\"] = logits[num_examples:][loss_mask[num_examples:]].mean()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272de2c3d92a43cdb0f419956d46a841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device count: 2\n",
      "Device 0 name: NVIDIA GeForce RTX 3080\n",
      "Device 1 name: NVIDIA GeForce RTX 3070\n",
      "Current device: 0\n",
      "cuda:0\n",
      "{'model.embed_tokens': 0, 'model.layers.0': 1, 'model.layers.1': 1, 'model.layers.2': 1, 'model.layers.3': 1, 'model.layers.4': 1, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 'cpu', 'model.layers.11': 'cpu', 'model.layers.12': 'cpu', 'model.layers.13': 'cpu', 'model.layers.14': 'cpu', 'model.layers.15': 'cpu', 'model.layers.16': 'cpu', 'model.layers.17': 'cpu', 'model.layers.18': 'cpu', 'model.layers.19': 'cpu', 'model.layers.20': 'cpu', 'model.layers.21': 'cpu', 'model.layers.22': 'cpu', 'model.layers.23': 'cpu', 'model.layers.24': 'cpu', 'model.layers.25': 'cpu', 'model.layers.26': 'cpu', 'model.layers.27': 'cpu', 'model.norm': 'cpu', 'model.rotary_emb': 'cpu', 'lm_head': 'cpu'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function add_logps at 0x7f6832b153a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb435b45e624ca7a9bf68ca01da7ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device_map = {\n",
    "    \"model.embed_tokens\": 0,\n",
    "    \"model.layers.0\": 1,\n",
    "    \"model.layers.1\": 1,\n",
    "    \"model.layers.2\": 1,\n",
    "    \"model.layers.3\": 1,\n",
    "    \"model.layers.4\": 1,\n",
    "    \"model.layers.5\": 1,\n",
    "    \"model.layers.6\": 1,\n",
    "    \"model.layers.7\": 1,\n",
    "    \"model.layers.8\": 1,\n",
    "    \"model.layers.9\": 1,\n",
    "    \"model.layers.10\": \"cpu\",\n",
    "    \"model.layers.11\": \"cpu\",\n",
    "    \"model.layers.12\": \"cpu\",\n",
    "    \"model.layers.13\": \"cpu\",\n",
    "    \"model.layers.14\": \"cpu\",\n",
    "    \"model.layers.15\": \"cpu\",\n",
    "    \"model.layers.16\": \"cpu\",\n",
    "    \"model.layers.17\": \"cpu\",\n",
    "    \"model.layers.18\": \"cpu\",\n",
    "    \"model.layers.19\": \"cpu\",\n",
    "    \"model.layers.20\": \"cpu\",\n",
    "    \"model.layers.21\": \"cpu\",\n",
    "    \"model.layers.22\": \"cpu\",\n",
    "    \"model.layers.23\": \"cpu\",\n",
    "    \"model.layers.24\": \"cpu\",\n",
    "    \"model.layers.25\": \"cpu\",\n",
    "    \"model.layers.26\": \"cpu\",\n",
    "    \"model.layers.27\": \"cpu\",\n",
    "    \"model.norm\": \"cpu\",\n",
    "    \"model.rotary_emb\": \"cpu\",\n",
    "    \"lm_head\": \"cpu\"\n",
    "}\n",
    "import torch\n",
    "sometokens = incodedClean[0]['combined_tokens']\n",
    "somerejectedtokens = incodedClean[0]['combined_rejected_tokens']\n",
    "decoded = tokenizer.decode(sometokens['input_ids'])\n",
    "model_id = \"Hankbeasley/PolycrestSFT-Qwen-7B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device_map,\n",
    "    #max_memory={0: \"2GiB\", 1: \"4.5GiB\", \"cpu\": \"50GiB\"},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "for idx in range(torch.cuda.device_count()):\n",
    "    print(f\"Device {idx} name:\", torch.cuda.get_device_name(idx))\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "\n",
    "print(model.device)\n",
    "print(model.hf_device_map)\n",
    "\n",
    "\n",
    "def add_logps(example):\n",
    "    output = generates(\n",
    "        model, \n",
    "        example['combined_tokens']['input_ids'], \n",
    "        example['combined_tokens']['attention_mask']\n",
    "    )\n",
    "    loss = getloss(\n",
    "        output,\n",
    "        example['combined_tokens']['input_ids'], \n",
    "        example['prompt_tokens']['attention_mask'], \n",
    "        example['chosen_tokens']['attention_mask'], \n",
    "        model.device\n",
    "    )\n",
    "    # Convert to list so it can be saved in the Arrow table\n",
    "    example['chosen_logps'] = loss['chosen_logps'].detach().cpu().tolist()\n",
    "    example['mean_chosen_logits'] = loss['mean_chosen_logits'].detach().cpu().tolist()\n",
    "    del output\n",
    "    del loss\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    output = generates(\n",
    "        model, \n",
    "        example['combined_rejected_tokens']['input_ids'], \n",
    "        example['combined_rejected_tokens']['attention_mask']\n",
    "    )\n",
    "    loss = getloss(\n",
    "        output,\n",
    "        example['combined_rejected_tokens']['input_ids'], \n",
    "        example['prompt_tokens']['attention_mask'], \n",
    "        example['rejected_tokens']['attention_mask'], \n",
    "        model.device\n",
    "    )\n",
    "    example['rejected_logps'] = loss['chosen_logps'].detach().cpu().tolist()\n",
    "    example['mean_rjected_logits'] = loss['mean_chosen_logits'].detach().cpu().tolist()\n",
    "    del output\n",
    "    del loss\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return example\n",
    "\n",
    "incodedClean = incodedClean.select(range(3)).map(add_logps)\n",
    "\n",
    "\n",
    "# for idx, example in enumerate(incodedClean):\n",
    "#     if idx >= 5:\n",
    "#         break\n",
    "#     print(f\"Processing example {idx}...\")\n",
    "#     # if (len(example['combined_tokens']['input_ids']) > 4000):\n",
    "#     #     continue\n",
    "#     print(example)\n",
    "#     print (len(example['combined_tokens']['input_ids'] ))\n",
    "    \n",
    "#     output = generates(model, example['combined_tokens']['input_ids'], example['combined_tokens']['attention_mask'])\n",
    "#     loss = getloss(output, example['combined_tokens']['input_ids'], example['prompt_tokens']['attention_mask'], example['chosen_tokens']['attention_mask'], model.device)\n",
    "#     print(loss)\n",
    "#     example['chosen_logps'] = loss['chosen_logps'].detach().cpu().tolist()\n",
    "    \n",
    "\n",
    "#     del output\n",
    "#     del loss\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "#     output = generates(model, example['combined_rejected_tokens']['input_ids'], example['combined_rejected_tokens']['attention_mask'])\n",
    "#     loss = getloss(output, example['combined_rejected_tokens']['input_ids'], example['prompt_tokens']['attention_mask'], example['rejected_tokens']['attention_mask'], model.device)\n",
    "#     print(loss)\n",
    "#     example['rejected_logps'] = loss['chosen_logps'].detach().cpu().tolist()\n",
    "#     print(example['chosen_logps'])\n",
    "#     print(example)\n",
    "\n",
    "\n",
    "#     del output\n",
    "#     del loss\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "    \n",
    "#model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "##with torch.inference_mode():\n",
    "\n",
    "    # print(model.hf_device_map)\n",
    "    # input_ids = torch.tensor(sometokens['input_ids']).unsqueeze(0).to(model.device)\n",
    "    # attention_mask = torch.tensor(sometokens['attention_mask']).unsqueeze(0).to(model.device)\n",
    "    # rejected_input_ids = torch.tensor(somerejectedtokens['input_ids']).unsqueeze(0).to(model.device)\n",
    "    # rejected_attention_mask = torch.tensor(somerejectedtokens['attention_mask']).unsqueeze(0).to(model.device)\n",
    "\n",
    "    # out = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # print(out)\n",
    "    # print(decoded)\n",
    "    # # Delete tensor explicitly\n",
    "    # del input_ids\n",
    "    # del attention_mask\n",
    "    # if torch.cuda.is_available():\n",
    "    #     torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n",
      "rejected_logps for example 0: None\n"
     ]
    }
   ],
   "source": [
    "for idx, example in enumerate(incodedClean):\n",
    "    print(\"rejected_logps for example 0:\", example.get('chosen_logps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(sometokens['input_ids']).unsqueeze(0)\n",
    "prompt_attention_mask = torch.tensor(ptokens[0]['attention_mask']).unsqueeze(0)\n",
    "completion_attention_mask =torch.tensor(incodedClean['chosen_tokens'][0]['attention_mask']).unsqueeze(0)\n",
    "logits = out.logits\n",
    "loss_mask = torch.cat(\n",
    "                (torch.zeros_like(prompt_attention_mask), completion_attention_mask),\n",
    "                dim=1,\n",
    "            )\n",
    "labels = torch.roll(input_ids, shifts=-1, dims=1).to(model.device)\n",
    "loss_mask = torch.roll(loss_mask, shifts=-1, dims=1).bool().to(model.device)\n",
    "per_token_logps = selective_log_softmax(logits, labels)\n",
    "loss_mask = loss_mask.to(logits.device)  # Ensure same device\n",
    "masked_logps = per_token_logps * loss_mask  \n",
    "total_loss = masked_logps.sum() / loss_mask.sum()  # Average over non-padding tokens\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "print(f\"Average loss: {total_loss:.4f}\")\n",
    "print(f\"Min logp at token index: {torch.argmin(masked_logps[0])}, token: {tokenizer.decode(predicted_ids[0,torch.argmin(masked_logps[0]) -1  ])} , {predicted_ids[0,torch.argmin(masked_logps[0])]}\")  \n",
    "print(f\"Min logp at token index: {torch.argmin(masked_logps[0])}, token: {tokenizer.decode(input_ids[0,torch.argmin(masked_logps[0]) -1 ])} , {input_ids[0,torch.argmin(masked_logps[0])]}\")  \n",
    "print(f\"Max logp at token index: {torch.argmax(masked_logps[0])}, token: {tokenizer.decode(predicted_ids[0,torch.argmax(masked_logps[0])])}\")\n",
    "print(f\"Max logp: {masked_logps.max():.4f}\")\n",
    "print(f\"Min logp: {masked_logps.min():.4f}\")\n",
    "\n",
    "\n",
    "lossoutput = loss_mask[0] * input_ids[0].to(model.device)\n",
    "decoded = tokenizer.decode(lossoutput)\n",
    "print(decoded)\n",
    "\n",
    "lossoutput = loss_mask[0] * predicted_ids[0].to(model.device)\n",
    "decoded = tokenizer.decode(lossoutput)\n",
    "print(decoded)\n",
    "\n",
    "# Optionally decode the predicted token ids to string tokens (assuming batch size of 1)\n",
    "predicted_tokens = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "per_token_logps[~loss_mask] = 0\n",
    "per_token_logps = torch.roll(per_token_logps, shifts=1, dims=1)\n",
    "all_logps = per_token_logps.sum(-1)\n",
    "num_examples = 1 \n",
    "output = {}\n",
    "output[\"chosen_logps\"] = all_logps[:num_examples]\n",
    "output[\"rejected_logps\"] = all_logps[num_examples:]\n",
    "output[\"mean_chosen_logits\"] = logits[:num_examples][loss_mask[:num_examples]].mean()\n",
    "output[\"mean_rejected_logits\"] = logits[num_examples:][loss_mask[num_examples:]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = torch.tensor(sometokens['input_ids']).unsqueeze(0).to(model.device)\n",
    "input_ids = torch.tensor(sometokens['input_ids']).unsqueeze(0).to(model.device)\n",
    "attention_mask = torch.tensor(sometokens['attention_mask']).unsqueeze(0).to(model.device)\n",
    "# out = model.generate(\n",
    "#     input_ids,\n",
    "#     max_length=3096,\n",
    "#     attention_mask=attention_mask,\n",
    "#     pad_token_id=tokenizer.pad_token_id\n",
    "# )\n",
    "out = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "print(out)\n",
    "print(decoded)\n",
    "# Delete tensor explicitly\n",
    "del input_ids\n",
    "\n",
    "# Clear CUDA cache if using GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdecoded = tokenizer.batch_decode(out)\n",
    "print(outputdecoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sometokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get lengths of each item in train['prompt_input_ids']\n",
    "lengths = [len(x) for x in train['prompt_input_ids']]\n",
    "\n",
    "# Print some basic statistics\n",
    "print(f\"Min length: {min(lengths)}\")\n",
    "print(f\"Max length: {max(lengths)}\")\n",
    "print(f\"Average length: {sum(lengths)/len(lengths):.2f}\")\n",
    "\n",
    "# Count frequency of different lengths\n",
    "length_counts = Counter(lengths)\n",
    "\n",
    "# Print top 5 most common lengths\n",
    "print(\"\\nMost common lengths:\")\n",
    "for length, count in length_counts.most_common(5):\n",
    "    print(f\"Length {length}: {count} items\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
